{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from from_root import from_root\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_FILE = f\"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\"\n",
    "\n",
    "log_dir = 'logs'\n",
    "\n",
    "logs_path = os.path.join(log_dir, LOG_FILE)\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=logs_path,\n",
    "    format=\"[ %(asctime)s ] %(name)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def error_message_detail(error, error_detail:sys):\n",
    "    _, _, exc_tb = error_detail.exc_info()\n",
    "    file_name = exc_tb.tb_frame.f_code.co_filename\n",
    "    error_message = \"Error occurred python script name [{0}] line number [{1}] error message [{2}]\".format(\n",
    "        file_name, exc_tb.tb_lineno, str(error)\n",
    "    )\n",
    "\n",
    "    return error_message\n",
    "\n",
    "class Credit_card_Exception(Exception):\n",
    "    def __init__(self, error_message, error_detail):\n",
    "        \"\"\"\n",
    "        :param error_message: error message in string format\n",
    "        \"\"\"\n",
    "        super().__init__(error_message)\n",
    "        self.error_message = error_message_detail(\n",
    "            error_message, error_detail=error_detail\n",
    "        )\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.error_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import dill\n",
    "import yaml\n",
    "from pandas import DataFrame\n",
    "\n",
    "from Demo_project.exception import Credit_card_Exception\n",
    "from Demo_project.logger import logging\n",
    "\n",
    "\n",
    "def read_yaml_file(file_path: str) -> dict:\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as yaml_file:\n",
    "            return yaml.safe_load(yaml_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "def write_yaml_file(file_path: str, content: object, replace: bool = False) -> None:\n",
    "    try:\n",
    "        if replace:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        with open(file_path, \"w\") as file:\n",
    "            yaml.dump(content, file)\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "def load_object(file_path: str) -> object:\n",
    "    logging.info(\"Entered the load_object method of utils\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(file_path, \"rb\") as file_obj:\n",
    "            obj = dill.load(file_obj)\n",
    "\n",
    "        logging.info(\"Exited the load_object method of utils\")\n",
    "\n",
    "        return obj\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "def save_numpy_array_data(file_path: str, array: np.array):\n",
    "    \"\"\"\n",
    "    Save numpy array data to file\n",
    "    file_path: str location of file to save\n",
    "    array: np.array data to save\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(file_path, 'wb') as file_obj:\n",
    "            np.save(file_obj, array)\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "def load_numpy_array_data(file_path: str) -> np.array:\n",
    "    \"\"\"\n",
    "    load numpy array data from file\n",
    "    file_path: str location of file to load\n",
    "    return: np.array data loaded\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file_obj:\n",
    "            return np.load(file_obj)\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "def save_object(file_path: str, obj: object) -> None:\n",
    "    logging.info(\"Entered the save_object method of utils\")\n",
    "\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        with open(file_path, \"wb\") as file_obj:\n",
    "            dill.dump(obj, file_obj)\n",
    "\n",
    "        logging.info(\"Exited the save_object method of utils\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "def drop_columns(df: DataFrame, cols: list)-> DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    drop the columns form a pandas DataFrame\n",
    "    df: pandas DataFrame\n",
    "    cols: list of columns to be dropped\n",
    "    \"\"\"\n",
    "    logging.info(\"Entered drop_columns methon of utils\")\n",
    "\n",
    "    try:\n",
    "        df = df.drop(columns=cols, axis=1)\n",
    "\n",
    "        logging.info(\"Exited the drop_columns method of utils\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.constant_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code part  writing in  constant file\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "DATABASE_NAME=\"demo_project_DB\"\n",
    "\n",
    "COLLECTION_NAME=\"fraud_data\"\n",
    "\n",
    "MONGODB_URL_KEY=\"MONGODB_URL\"\n",
    "\n",
    "PIPELINE_NAME:str =\"Demo_project\"  # NOT A SRC its pipeline name\n",
    "\n",
    "ARTIFICAT_DIR:str=\"artificat\"\n",
    "\n",
    "MODEL_FILE_NAME=\"model.pkl\"\n",
    "\n",
    "TARGET_COLUMN=\"default payment next month\"\n",
    "\n",
    "PREPROCESSING_OBJECT_FILE_NAME=\"preprocessing.pkl\"\n",
    "\n",
    "\n",
    "FILE_NAME:str=\"default of credit card data.xls\"\n",
    "\n",
    "TRAIN_FILE_NAME:str=\"train.xls\"\n",
    "\n",
    "TEST_FILE_NAME:str=\"test.xls\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Data Ingestion related constant start with DATA_INGESTION VAR NAME\n",
    "\"\"\"\n",
    "DATA_INGESTION_COLLECTION_NAME: str = \"fraud_data\"\n",
    "DATA_INGESTION_DIR_NAME: str = \"data_ingestion\"\n",
    "DATA_INGESTION_FEATURE_STORE_DIR: str = \"feature_store\"\n",
    "DATA_INGESTION_INGESTED_DIR: str = \"ingested\"\n",
    "DATA_INGESTION_TRAIN_TEST_SPLIT_RATIO: float = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating MongoDBClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in configuration mongo_db_connection.py\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pymongo\n",
    "import certifi\n",
    "from Demo_project.exception import Credit_card_Exception\n",
    "from Demo_project.logger import logging\n",
    "from Demo_project.constants import DATABASE_NAME, MONGODB_URL_KEY\n",
    "\n",
    "ca = certifi.where()\n",
    "\n",
    "class MongoDBClient:\n",
    "    \"\"\"\n",
    "    Class Name :   export_data_into_feature_store\n",
    "    Description :   This method exports the dataframe from mongodb feature store as dataframe \n",
    "    \n",
    "    Output      :   connection to mongodb database\n",
    "    On Failure  :   raises an exception\n",
    "    \"\"\"\n",
    "    client = None\n",
    "\n",
    "    def __init__(self, database_name=DATABASE_NAME) -> None:\n",
    "        try:\n",
    "            if MongoDBClient.client is None:\n",
    "                mongo_db_url = os.getenv(MONGODB_URL_KEY)\n",
    "                if mongo_db_url is None:\n",
    "                    raise Exception(f\"Environment key: {MONGODB_URL_KEY} not set.\")\n",
    "                MongoDBClient.client = pymongo.MongoClient(mongo_db_url,tlsCAFile=ca)\n",
    "            self.client= MongoDBClient.client\n",
    "            self.database = self.client[database_name]\n",
    "            self.database_name = database_name\n",
    "            logging.info(\"Connected to MongoDB database successfull\")\n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e,sys) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: Error occurred python script name [c:\\demo\\demo_project\\Demo_project\\configuration\\mongo_db_connection.py] line number[28] error message [Environment key: MONGODB_URL not set.]\n"
     ]
    }
   ],
   "source": [
    "from Demo_project.configuration.mongo_db_connection import MongoDBClient\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize MongoDB client\n",
    "        mongo_client = MongoDBClient()  # Default uses DATABASE_NAME\n",
    "\n",
    "        # Print connection details for confirmation\n",
    "        print(f\"Connected to MongoDB database: {mongo_client.databse_name}\")\n",
    "        print(f\"Client: {mongo_client.client}\")\n",
    "        \n",
    "        # Accessing a specific collection (example)\n",
    "        collection_name = \"fraud_data\"  # Replace with your collection name\n",
    "        collection = mongo_client.databse[collection_name]\n",
    "        print(f\"Connected to collection: {collection_name}\")\n",
    "\n",
    "        # Fetching and displaying documents from the collection\n",
    "        documents = collection.find()\n",
    "        print(\"Documents in the collection:\")\n",
    "        for doc in documents:\n",
    "            print(doc)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from us_visa.constants import TARGET_COLUMN, SCHEMA_FILE_PATH, CURRENT_YEAR\n",
    "from us_visa.entity.config_entity import DataTransformationConfig\n",
    "from us_visa.entity.artifact_entity import DataTransformationArtifact, DataIngestionArtifact, DataValidationArtifact\n",
    "from us_visa.exception import USvisaException\n",
    "from us_visa.logger import logging\n",
    "from us_visa.utils.main_utils import save_object, save_numpy_array_data, read_yaml_file, drop_columns\n",
    "from us_visa.entity.estimator import TargetValueMapping\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, data_ingestion_artifact: DataIngestionArtifact,\n",
    "                 data_transformation_config: DataTransformationConfig,\n",
    "                 data_validation_artifact: DataValidationArtifact):\n",
    "        \"\"\"\n",
    "        :param data_ingestion_artifact: Output reference of data ingestion artifact stage\n",
    "        :param data_transformation_config: Configuration for data transformation\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_transformation_config = data_transformation_config\n",
    "            self.data_validation_artifact = data_validation_artifact\n",
    "            self._schema_config = read_yaml_file(file_path=SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(file_path) -> pd.DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_to_zero(df, columns):\n",
    "        \"\"\"\n",
    "        Replace invalid values (-2, -1, 0) in specified columns with 0.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for col in columns:\n",
    "                fil = (df[col] == -2) | (df[col] == -1) | (df[col] == 0)\n",
    "                df.loc[fil, col] = 0\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_outliers(df, columns):\n",
    "        \"\"\"\n",
    "        Remove outliers using IQR method for specified columns.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for col in columns:\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_vif(df, numerical_columns):\n",
    "        \"\"\"\n",
    "        Calculate Variance Inflation Factor (VIF) for numerical columns.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            X = df[numerical_columns]\n",
    "            vif_data = pd.DataFrame()\n",
    "            vif_data[\"Feature\"] = X.columns\n",
    "            vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "            return vif_data\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "        \"\"\"\n",
    "        Method to create and return a data transformer object for the data.\n",
    "        \"\"\"\n",
    "        logging.info(\"Entered get_data_transformer_object method of DataTransformation class\")\n",
    "\n",
    "        try:\n",
    "            # Pipeline for numerical columns\n",
    "            numeric_transformer = Pipeline(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),  # Impute missing values for numerical columns\n",
    "                (\"scaler\", StandardScaler())  # Scaling numerical features\n",
    "            ])\n",
    "\n",
    "            # Pipeline for categorical columns\n",
    "            oh_transformer = Pipeline(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Impute missing values for categorical columns\n",
    "                (\"one_hot_encoder\", OneHotEncoder()),  # One-hot encoding for categorical features\n",
    "                (\"scaler\", StandardScaler(with_mean=False))  # Scaling categorical features\n",
    "            ])\n",
    "            \n",
    "            # Ordinal encoding for specified columns\n",
    "            ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "            # Get columns from schema configuration\n",
    "            oh_columns = self._schema_config['oh_columns']\n",
    "            or_columns = self._schema_config['or_columns']\n",
    "            transform_columns = self._schema_config['transform_columns']\n",
    "            num_features = self._schema_config['num_features']\n",
    "\n",
    "            # Power Transformer for feature transformation\n",
    "            transform_pipe = Pipeline(steps=[('transformer', PowerTransformer(method='yeo-johnson'))])\n",
    "\n",
    "            # Combining all transformations using ColumnTransformer\n",
    "            preprocessor = ColumnTransformer([\n",
    "                (\"OneHotEncoder\", oh_transformer, oh_columns),\n",
    "                (\"Ordinal_Encoder\", ordinal_encoder, or_columns),\n",
    "                (\"Transformer\", transform_pipe, transform_columns),\n",
    "                (\"StandardScaler\", numeric_transformer, num_features)\n",
    "            ])\n",
    "\n",
    "            logging.info(\"Created preprocessor object from ColumnTransformer\")\n",
    "            return preprocessor\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    def initiate_data_transformation(self) -> DataTransformationArtifact:\n",
    "        \"\"\"\n",
    "        Method to initiate the data transformation component for the pipeline.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.data_validation_artifact.validation_status:\n",
    "                logging.info(\"Starting data transformation\")\n",
    "\n",
    "                # Read and clean data\n",
    "                train_df = DataTransformation.read_data(file_path=self.data_ingestion_artifact.trained_file_path)\n",
    "                test_df = DataTransformation.read_data(file_path=self.data_ingestion_artifact.test_file_path)\n",
    "\n",
    "                # Replace invalid values in specified columns\n",
    "                target_columns = self._schema_config['replace_invalid_columns']\n",
    "                train_df = self.replace_to_zero(train_df, target_columns)\n",
    "                test_df = self.replace_to_zero(test_df, target_columns)\n",
    "\n",
    "                # Remove duplicates from both train and test data\n",
    "                train_df = train_df.drop_duplicates()\n",
    "                test_df = test_df.drop_duplicates()\n",
    "\n",
    "                # Remove outliers from numerical columns\n",
    "                numerical_columns = self._schema_config['num_features']\n",
    "                train_df = self.remove_outliers(train_df, numerical_columns)\n",
    "                test_df = self.remove_outliers(test_df, numerical_columns)\n",
    "\n",
    "                # VIF Analysis for Multicollinearity Check\n",
    "                vif_data = self.calculate_vif(train_df, numerical_columns)\n",
    "                vif_threshold = 5.0\n",
    "                selected_features = vif_data[vif_data[\"VIF\"] <= vif_threshold][\"Feature\"].tolist()\n",
    "\n",
    "                # Retaining only selected features\n",
    "                train_df = train_df[selected_features + [TARGET_COLUMN]]\n",
    "                test_df = test_df[selected_features + [TARGET_COLUMN]]\n",
    "\n",
    "                # Getting preprocessor object\n",
    "                preprocessor = self.get_data_transformer_object()\n",
    "\n",
    "                # Separating features and target\n",
    "                input_feature_train_df = train_df.drop(columns=[TARGET_COLUMN], axis=1)\n",
    "                target_feature_train_df = train_df[TARGET_COLUMN]\n",
    "\n",
    "                input_feature_test_df = test_df.drop(columns=[TARGET_COLUMN], axis=1)\n",
    "                target_feature_test_df = test_df[TARGET_COLUMN]\n",
    "\n",
    "                # Apply transformations to the train and test data\n",
    "                input_feature_train_arr = preprocessor.fit_transform(input_feature_train_df)\n",
    "                input_feature_test_arr = preprocessor.transform(input_feature_test_df)\n",
    "\n",
    "                # Apply SMOTEENN to handle class imbalance\n",
    "                smt = SMOTEENN(sampling_strategy=\"minority\")\n",
    "                input_feature_train_final, target_feature_train_final = smt.fit_resample(\n",
    "                    input_feature_train_arr, target_feature_train_df\n",
    "                )\n",
    "                input_feature_test_final, target_feature_test_final = smt.fit_resample(\n",
    "                    input_feature_test_arr, target_feature_test_df\n",
    "                )\n",
    "\n",
    "                # Create final train and test arrays\n",
    "                train_arr = np.c_[input_feature_train_final, np.array(target_feature_train_final)]\n",
    "                test_arr = np.c_[input_feature_test_final, np.array(target_feature_test_final)]\n",
    "\n",
    "                # Save preprocessed data and objects\n",
    "                save_object(self.data_transformation_config.transformed_object_file_path, preprocessor)\n",
    "                save_numpy_array_data(self.data_transformation_config.transformed_train_file_path, array=train_arr)\n",
    "                save_numpy_array_data(self.data_transformation_config.transformed_test_file_path, array=test_arr)\n",
    "\n",
    "                # Return the transformation artifact\n",
    "                return DataTransformationArtifact(\n",
    "                    transformed_object_file_path=self.data_transformation_config.transformed_object_file_path,\n",
    "                    transformed_train_file_path=self.data_transformation_config.transformed_train_file_path,\n",
    "                    transformed_test_file_path=self.data_transformation_config.transformed_test_file_path\n",
    "                )\n",
    "            else:\n",
    "                raise Exception(self.data_validation_artifact.message)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataValidation.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.api.types import is_dtype_equal\n",
    "from typing import Dict, Any ,Tuple\n",
    "from evidently.model_profile import Profile\n",
    "from evidently.model_profile.sections import DataDriftProfileSection\n",
    "\n",
    "from Demo_project.exception import Credit_card_Exception\n",
    "from Demo_project.logger import logging\n",
    "from Demo_project.utils.main_utils import read_yaml_file, write_yaml_file\n",
    "from Demo_project.entity.artifact_entity import DataIngestionArtifact, DataValidationArtifact\n",
    "from Demo_project.entity.config_entity import DataValidationConfig\n",
    "from Demo_project.constants import SCHEMA_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation:\n",
    "    def __init__(self, data_ingestion_artifact: DataIngestionArtifact, data_validation_config: DataValidationConfig):\n",
    "        \"\"\"\n",
    "        :param data_ingestion_artifact: Output reference of data ingestion artifact stage\n",
    "        :param data_validation_config: configuration for data validation\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_validation_config = data_validation_config\n",
    "            self._schema_config =read_yaml_file(file_path=SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e,sys)\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_data(file_path) -> DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_types(self, dataframe: DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Method Name :   check_data_types\n",
    "        Description :   This method validates if the data types of the columns match the schema\n",
    "        Output      :   Returns bool value based on validation results\n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        try:\n",
    "            schema_col_dtype = self._schema_config.get(\"dtypes\",{})\n",
    "            print(schema_col_dtype)\n",
    "            schema_dtype=list(schema_col_dtype.values())\n",
    "            schema_col=list(schema_col_dtype.keys())\n",
    "            df_dtype=[dtype.name for dtype in dataframe.dtypes]\n",
    "            df_col=list(dataframe.columns)\n",
    "            \n",
    "            mismatched_columns = []\n",
    "\n",
    "            for column in schema_col:\n",
    "                if column in df_col:\n",
    "                    logging.info(f\"schema column: {column}  is present in dataframe\")\n",
    "                \n",
    "                else:\n",
    "                    mismatched_columns.append(f\"{column} is missing in the DataFrame.\")\n",
    "                    logging.info(f\"Column not found in dataframe: {column}\")\n",
    "                    continue\n",
    "                    \n",
    "            mismatched_dtypes = []        \n",
    "\n",
    "            for dtype in schema_dtype:\n",
    "                if dtype  in df_dtype:   \n",
    "                    logging.info(f\"schema dtype : {dtype}  is present in dataframe dtype\")\n",
    "                else:\n",
    "                    mismatched_dtypes.append(f\"Data type mismatch for column: {column} , Found: {dtype}\")\n",
    "\n",
    "            if len(mismatched_columns)>0:\n",
    "                logging.info(f\"schema data column mismatched with dataframe_columns.\")\n",
    "\n",
    "            if len(mismatched_dtypes)>0:\n",
    "                logging.info(f\"schema data type mismatched with dataframe_dtypes.\")\n",
    "\n",
    "            return False if len(mismatched_columns)>0 or len(mismatched_dtypes)>0 else True\n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': 'int64', 'LIMIT_BAL': 'int64', 'SEX': 'int64', 'EDUCATION': 'int64', 'MARRIAGE': 'int64', 'AGE': 'int64', 'PAY_0': 'int64', 'PAY_2': 'int64', 'PAY_3': 'int64', 'PAY_4': 'int64', 'PAY_5': 'int64', 'PAY_6': 'int64', 'BILL_AMT1': 'int64', 'BILL_AMT2': 'int64', 'BILL_AMT3': 'int64', 'BILL_AMT4': 'int64', 'BILL_AMT5': 'int64', 'BILL_AMT6': 'int64', 'PAY_AMT1': 'int64', 'PAY_AMT2': 'int64', 'PAY_AMT3': 'int64', 'PAY_AMT4': 'int64', 'PAY_AMT5': 'int64', 'PAY_AMT6': 'int64', 'default payment next month': 'int64'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get expected at least 1 argument, got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m dataframe_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(df_col, df_dtype))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataframe_schema)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataframe_schema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Compare schema_data with dataframe_schema\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompare_schemas\u001b[39m(schema1, schema2):\n",
      "\u001b[1;31mTypeError\u001b[0m: get expected at least 1 argument, got 0"
     ]
    }
   ],
   "source": [
    "# Provided schema and dataframe metadata\n",
    "schema_data = {\n",
    "    'ID': 'int64', 'LIMIT_BAL': 'int64', 'SEX': 'int64', 'EDUCATION': 'int64', 'MARRIAGE': 'int64', \n",
    "    'AGE': 'int64', 'PAY_0': 'int64', 'PAY_2': 'int64', 'PAY_3': 'int64', 'PAY_4': 'int64', \n",
    "    'PAY_5': 'int64', 'PAY_6': 'int64', 'BILL_AMT1': 'int64', 'BILL_AMT2': 'int64', \n",
    "    'BILL_AMT3': 'int64', 'BILL_AMT4': 'int64', 'BILL_AMT5': 'int64', 'BILL_AMT6': 'int64', \n",
    "    'PAY_AMT1': 'int64', 'PAY_AMT2': 'int64', 'PAY_AMT3': 'int64', 'PAY_AMT4': 'int64', \n",
    "    'PAY_AMT5': 'int64', 'PAY_AMT6': 'int64', 'default payment next month': 'int64'\n",
    "}\n",
    "\n",
    "df_col = [\n",
    "    'ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', \n",
    "    'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', \n",
    "    'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'default payment next month'\n",
    "]\n",
    "\n",
    "df_dtype = [\n",
    "    'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', \n",
    "    'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', \n",
    "    'int64', 'int64', 'int64', 'int64', 'int64'\n",
    "]\n",
    "\n",
    "# Create a dictionary from df_col and df_dtype\n",
    "dataframe_schema = dict(zip(df_col, df_dtype))\n",
    "print(dataframe_schema)\n",
    "print(dataframe_schema.items())\n",
    "\n",
    "# Compare schema_data with dataframe_schema\n",
    "def compare_schemas(schema1, schema2):\n",
    "    mismatched_columns = []\n",
    "    extra_columns = []\n",
    "    missing_columns = []\n",
    "\n",
    "    # Check for mismatched or missing columns\n",
    "    for column, dtype in schema1.items():\n",
    "        if column not in schema2:\n",
    "            missing_columns.append(column)\n",
    "        elif schema2[column] != dtype:\n",
    "            mismatched_columns.append(f\"{column}: Expected {dtype}, but got {schema2[column]}\")\n",
    "\n",
    "    # Check for extra columns in schema2\n",
    "    for column in schema2.keys():\n",
    "        if column not in schema1:\n",
    "            extra_columns.append(column)\n",
    "\n",
    "    return mismatched_columns, missing_columns, extra_columns\n",
    "\n",
    "\n",
    "# Perform the comparison\n",
    "mismatched, missing, extra = compare_schemas(schema_data, dataframe_schema)\n",
    "\n",
    "# Display results\n",
    "if mismatched:\n",
    "    print(\"Mismatched columns:\")\n",
    "    for mismatch in mismatched:\n",
    "        print(mismatch)\n",
    "\n",
    "if missing:\n",
    "    print(\"\\nMissing columns:\")\n",
    "    for miss in missing:\n",
    "        print(miss)\n",
    "\n",
    "if extra:\n",
    "    print(\"\\nExtra columns:\")\n",
    "    for ext in extra:\n",
    "        print(ext)\n",
    "\n",
    "if not mismatched and not missing and not extra:\n",
    "    print(\"Schemas match perfectly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Provided schema and dataframe metadata\n",
    "schema_data = {\n",
    "    'ID': 'int64', 'LIMIT_BAL': 'int64', 'SEX': 'int64', 'EDUCATION': 'int64', 'MARRIAGE': 'int64', \n",
    "    'AGE': 'int64', 'PAY_0': 'int64', 'PAY_2': 'int64', 'PAY_3': 'int64', 'PAY_4': 'int64', \n",
    "    'PAY_5': 'int64', 'PAY_6': 'int64', 'BILL_AMT1': 'int64', 'BILL_AMT2': 'int64', \n",
    "    'BILL_AMT3': 'int64', 'BILL_AMT4': 'int64', 'BILL_AMT5': 'int64', 'BILL_AMT6': 'int64', \n",
    "    'PAY_AMT1': 'int64', 'PAY_AMT2': 'int64', 'PAY_AMT3': 'int64', 'PAY_AMT4': 'int64', \n",
    "    'PAY_AMT5': 'int64', 'PAY_AMT6': 'int64', 'default payment next month': 'int64'\n",
    "}\n",
    "\n",
    "df_col = [\n",
    "    'ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', \n",
    "    'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', \n",
    "    'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'default payment next month'\n",
    "]\n",
    "\n",
    "df_dtype = [\n",
    "    'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', \n",
    "    'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', \n",
    "    'int64', 'int64', 'int64', 'int64', 'int64'\n",
    "]\n",
    "\n",
    "# Create a dictionary from df_col and df_dtype\n",
    "dataframe_schema = dict(zip(df_col, df_dtype))\n",
    "#print(dataframe_schema)\n",
    "print()\n",
    "#print(dataframe_schema.items())\n",
    "mismatched_columns = []\n",
    "extra_columns = []\n",
    "missing_columns = []\n",
    "\n",
    " # Check for mismatched or missing columns\n",
    "for column, dtype in schema_data.items():\n",
    "        if column not in dataframe_schema:\n",
    "            missing_columns.append(column)\n",
    "        elif dataframe_schema[column] != dtype:\n",
    "            mismatched_columns.append(f\"{column}: Expected {dtype}, but got {dataframe_schema[column]}\")\n",
    "print(mismatched_columns)\n",
    "print(missing_columns)\n",
    "print(extra_columns)\n",
    "\n",
    "# Check for extra columns in schema2\n",
    "for column in dataframe_schema.keys():\n",
    "        if column not in schema_data:\n",
    "            extra_columns.append(column)\n",
    "\n",
    "print(extra_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_col_dtype = {\n",
    "    'ID': 'int64', 'LIMIT_BAL': 'int64', 'SEX': 'int64', 'EDUCATION': 'int64', 'MARRIAGE': 'int64', \n",
    "    'AGE': 'int64', 'PAY_0': 'int64', 'PAY_2': 'int64', 'PAY_3': 'int64', 'PAY_4': 'int64', \n",
    "    'PAY_5': 'int64', 'PAY_6': 'int64', 'BILL_AMT1': 'int64', 'BILL_AMT2': 'int64', \n",
    "    'BILL_AMT3': 'int64', 'BILL_AMT4': 'int64', 'BILL_AMT5': 'int64', 'BILL_AMT6': 'int64', \n",
    "    'PAY_AMT1': 'int64', 'PAY_AMT2': 'int64', 'PAY_AMT3': 'int64', 'PAY_AMT4': 'int64', \n",
    "    'PAY_AMT5': 'int64', 'PAY_AMT6': 'int64', 'default payment next month': 'int64'\n",
    "}\n",
    "df_col = [\n",
    "    'ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', \n",
    "    'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', \n",
    "    'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'default payment next month'\n",
    "]\n",
    "\n",
    "df_dtype = [\n",
    "    'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', \n",
    "    'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'int64', \n",
    "    'int64', 'int64', 'int64', 'int64', 'int64'\n",
    "]\n",
    "\n",
    "# Create a dictionary from df_col and df_dtype\n",
    "dataframe_schema = dict(zip(df_col, df_dtype))\n",
    "def check_data_types(self, dataframe: DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Method Name :   check_data_types\n",
    "    Description :   This method validates if the data types and columns of the dataframe match the schema\n",
    "    Output      :   Returns bool value based on validation results\n",
    "    On Failure  :   Write an exception log and then raise an exception\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve schema configuration for columns and their data types\n",
    "        #schema_col_dtype = self._schema_config.get(\"dtypes\", {})\n",
    "        logging.info(f\"Schema data types: {schema_col_dtype}\")\n",
    "        \n",
    "        # Extract schema column names and data types\n",
    "        schema_columns = list(schema_col_dtype.keys())\n",
    "        schema_dtypes = list(schema_col_dtype.values())\n",
    "        \n",
    "        # Extract DataFrame column names and their data types\n",
    "        #dataframe_columns = list(dataframe.columns)\n",
    "        #dataframe_dtypes = [dtype.name for dtype in dataframe.dtypes]\n",
    "\n",
    "        logging.info(f\"DataFrame columns: {df_col}\")\n",
    "        logging.info(f\"DataFrame data types: {df_dtype}\")\n",
    "\n",
    "        # Validate column presence\n",
    "        missing_columns = [col for col in schema_columns if col not in df_col]\n",
    "        if missing_columns:\n",
    "            logging.info(f\"Missing columns in DataFrame: {missing_columns}\")\n",
    "\n",
    "        # Validate data types\n",
    "        mismatched_dtypes = [\n",
    "            f\"Column: {col}, Expected: {expected_dtype}, Found: {dataframe[col].dtypes}\"\n",
    "            for col, expected_dtype in schema_col_dtype.items()\n",
    "            if col in df_col and str(dataframe[col].dtypes) != expected_dtype\n",
    "        ]\n",
    "        if mismatched_dtypes:\n",
    "            logging.info(f\"Data type mismatches: {mismatched_dtypes}\")\n",
    "\n",
    "        # Return False if any validation errors are found\n",
    "        if missing_columns or mismatched_dtypes:\n",
    "            logging.info(\"Schema validation failed.\")\n",
    "            return False\n",
    "\n",
    "        # If no issues, validation is successful\n",
    "        logging.info(\"Schema validation passed.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking data transformation\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "#from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from Demo_project.constants import TARGET_COLUMN, SCHEMA_FILE_PATH\n",
    "from Demo_project.entity.config_entity import DataTransformationConfig\n",
    "from Demo_project.entity.artifact_entity import DataTransformationArtifact, DataIngestionArtifact, DataValidationArtifact\n",
    "from Demo_project.exception import Credit_card_Exception\n",
    "from Demo_project.logger import logging\n",
    "from Demo_project.utils.main_utils import save_object, save_numpy_array_data, read_yaml_file, drop_columns\n",
    "#from Demo_project.entity.estimator import TargetValueMapping\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, data_ingestion_artifact: DataIngestionArtifact,\n",
    "                 data_transformation_config: DataTransformationConfig,\n",
    "                 data_validation_artifact: DataValidationArtifact):\n",
    "        \"\"\"\n",
    "        :param data_ingestion_artifact: Output reference of data ingestion artifact stage\n",
    "        :param data_transformation_config: Configuration for data transformation\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_transformation_config = data_transformation_config\n",
    "            self.data_validation_artifact = data_validation_artifact\n",
    "            self._schema_config = read_yaml_file(file_path=SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(file_path) -> pd.DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e, sys)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_to_zero(df, columns):\n",
    "        \"\"\"\n",
    "        Replace invalid values (-2, -1, 0) in specified columns with 0.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for col in columns:\n",
    "                fil = (df[col] == -2) | (df[col] == -1) | (df[col] == 0)\n",
    "                df.loc[fil, col] = 0\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e, sys)\n",
    "        \n",
    "    @staticmethod\n",
    "    def replace_education_values(df):\n",
    "        \"\"\"\n",
    "        Replace values 4, 5, 6 with 0 in the EDUCATION column.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df[\"EDUCATION\"] = df[\"EDUCATION\"].replace({4: 0, 5: 0, 6: 0})\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_marriage_values(df):\n",
    "        \"\"\"\n",
    "        Replace value 0 with 3 in the MARRIAGE column.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if \"MARRIAGE\" in df.columns:\n",
    "                df[\"MARRIAGE\"] = df[\"MARRIAGE\"].replace({0:3})\n",
    "            else:\n",
    "                raise ValueError(\"Column 'MARRIAGE' not found in DataFrame.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e, sys)\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    #@staticmethod\n",
    "    #def remove_outliers(df, columns):\n",
    "    #   \"\"\"\n",
    "    #    Remove outliers using IQR method for specified columns.\n",
    "    #   \"\"\"\n",
    "    #   try:\n",
    "    #       for col in columns:\n",
    "    #            Q1 = df[col].quantile(0.25)\n",
    "    #           Q3 = df[col].quantile(0.75)\n",
    "    #            IQR = Q3 - Q1\n",
    "    #            lower_bound = Q1 - 1.5 * IQR\n",
    "    #            upper_bound = Q3 + 1.5 * IQR\n",
    "    #            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    #       return df\n",
    "    #    except Exception as e:\n",
    "    #        raise Credit_card_Exception(e, sys)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_duplicates(df):\n",
    "        \"\"\"\n",
    "        Remove duplicate rows from the DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            #before_count = len(df.shape[0])\n",
    "            df = df.drop_duplicates(inplace=True)\n",
    "            #after_count = len(df.shape[0])\n",
    "            #logging.info(f\"Removed {before_count - after_count} duplicate rows.\")\n",
    "            logging.info(f\"Removed  duplicate rows.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e, sys)\n",
    "\n",
    "\n",
    "    def get_data_transformer_object(self) -> Pipeline:\n",
    "        \"\"\"\n",
    "        Method Name :   get_data_transformer_object\n",
    "        Description :   This method creates and returns a data transformer object for the data\n",
    "        \n",
    "        Output      :   data transformer object is created and returned \n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        logging.info(\n",
    "            \"Entered get_data_transformer_object method of DataTransformation class\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            logging.info(\"Got numerical cols from schema config\")\n",
    "\n",
    "            numeric_transformer = StandardScaler()\n",
    "\n",
    "             # follow below Pipeline for numerical columns if data has missing values \n",
    "\n",
    "            #numeric_transformer = Pipeline(steps=[\n",
    "            #    (\"imputer\", SimpleImputer(strategy=\"median\")),  # Impute missing values for numerical columns\n",
    "            #    (\"scaler\", StandardScaler())  # Scaling numerical features\n",
    "            #])\n",
    "\n",
    "            oh_transformer = OneHotEncoder()      # One-hot encoding for categorical features\n",
    "        \n",
    "            # follow below Pipeline for categorical columns if data has missing values\n",
    "\n",
    "            #oh_transformer = Pipeline(steps=[\n",
    "            #    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Impute missing values for categorical columns\n",
    "            #    (\"one_hot_encoder\", OneHotEncoder()),  # One-hot encoding for categorical features\n",
    "            #    (\"scaler\", StandardScaler(with_mean=False))  # Scaling categorical features\n",
    "            #])\n",
    "           \n",
    "\n",
    "            ordinal_encoder = OrdinalEncoder()  # Ordinal encoding for specified columns\n",
    "\n",
    "\n",
    "            logging.info(\"Initialized StandardScaler, OneHotEncoder, OrdinalEncoder\")\n",
    "\n",
    "\n",
    "\n",
    "            # Get columns from schema configuration\n",
    "            num_features = self._schema_config['num_features']\n",
    "            oh_columns = self._schema_config['oh_columns']\n",
    "            or_columns = self._schema_config['or_columns']\n",
    "            transform_columns = self._schema_config['transform_columns']\n",
    "\n",
    "\n",
    "            logging.info(\"Initialize PowerTransformer\")\n",
    "            transform_pipe = Pipeline(steps=[\n",
    "                ('transformer', PowerTransformer(method='yeo-johnson'))\n",
    "            ])\n",
    "\n",
    "            # Combining all transformations using ColumnTransformer\n",
    "            preprocessor = ColumnTransformer(\n",
    "                [\n",
    "                    (\"OneHotEncoder\", oh_transformer, oh_columns),\n",
    "                    (\"Ordinal_Encoder\", ordinal_encoder, or_columns),\n",
    "                    (\"Transformer\", transform_pipe, transform_columns),\n",
    "                    (\"StandardScaler\", numeric_transformer, num_features)\n",
    "                ]\n",
    "            )\n",
    "            logging.info(\"Created preprocessor object from ColumnTransformer\")\n",
    "\n",
    "            logging.info(\"Exited get_data_transformer_object method of DataTransformation class\")\n",
    "\n",
    "            return preprocessor\n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "    def initiate_data_transformation(self, ) -> DataTransformationArtifact:\n",
    "        \"\"\"\n",
    "        Method Name :   initiate_data_transformation\n",
    "        Description :   This method initiates the data transformation component for the pipeline \n",
    "        \n",
    "        Output      :   data transformer steps are performed and preprocessor object is created  \n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            if self.data_validation_artifact.validation_status:\n",
    "                logging.info(\"Starting data transformation\")\n",
    "                preprocessor = self.get_data_transformer_object()\n",
    "                logging.info(\"Got the preprocessor object\")\n",
    "\n",
    "                train_df = DataTransformation.read_data(file_path=self.data_ingestion_artifact.trained_file_path)\n",
    "                test_df = DataTransformation.read_data(file_path=self.data_ingestion_artifact.test_file_path)\n",
    "                \n",
    "                # Replace invalid values in specified columns\n",
    "                target_columns = self._schema_config['replace_invalid_values_in_columns']\n",
    "                train_df = self.replace_to_zero(train_df, target_columns)\n",
    "                test_df = self.replace_to_zero(test_df, target_columns)\n",
    "                \n",
    "                print(train_df.head())\n",
    "                print(test_df.head())\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "                train_df = self.replace_education_values(train_df)\n",
    "                test_df = self.replace_education_values(test_df)\n",
    "\n",
    "                train_df = self.replace_marriage_values(train_df)\n",
    "                test_df = self.replace_marriage_values(test_df)\n",
    "\n",
    "                train_df = self.remove_duplicates(train_df)\n",
    "                test_df = self.remove_duplicates(test_df)\n",
    "                print(type(train_df))\n",
    "\n",
    "                input_feature_train_df = train_df.drop(columns=[TARGET_COLUMN], axis=1)\n",
    "                target_feature_train_df = train_df[TARGET_COLUMN]\n",
    "\n",
    "                logging.info(\"Got train features and test features of Training dataset\")\n",
    "\n",
    "                drop_cols = self._schema_config['drop_columns']\n",
    "\n",
    "                logging.info(\"drop the columns in drop_cols of Training dataset\")\n",
    "\n",
    "                input_feature_train_df = drop_columns(df=input_feature_train_df, cols = drop_cols)\n",
    "                   \n",
    "\n",
    "                   #incase target column categorical, replace with target value mapping with numerical values from estimator.py\n",
    "                #target_feature_train_df = target_feature_train_df.replace(TargetValueMapping()._asdict())\n",
    "                target_feature_train_df = target_feature_train_df\n",
    "\n",
    "                input_feature_test_df = test_df.drop(columns=[TARGET_COLUMN], axis=1)\n",
    "\n",
    "                target_feature_test_df = test_df[TARGET_COLUMN]\n",
    "\n",
    "\n",
    "                input_feature_test_df = drop_columns(df=input_feature_test_df, cols = drop_cols)\n",
    "\n",
    "                logging.info(\"drop the columns in drop_cols of Test dataset\")\n",
    "                #incase target column categorical, replace with target value mapping with numerical values from estimator.py\n",
    "                #target_feature_test_df = target_feature_test_df.replace(TargetValueMapping()._asdict() )\n",
    "                target_feature_test_df = target_feature_test_df \n",
    "\n",
    "                logging.info(\"Got train features and test features of Testing dataset\")\n",
    "\n",
    "                logging.info(\n",
    "                    \"Applying preprocessing object on training dataframe and testing dataframe\"\n",
    "                )\n",
    "\n",
    "                input_feature_train_arr = preprocessor.fit_transform(input_feature_train_df)\n",
    "\n",
    "                logging.info(\n",
    "                    \"Used the preprocessor object to fit transform the train features\"\n",
    "                )\n",
    "\n",
    "                input_feature_test_arr = preprocessor.transform(input_feature_test_df)\n",
    "\n",
    "                logging.info(\"Used the preprocessor object to transform the test features\")\n",
    "\n",
    "                logging.info(\"Applying SMOTEENN on Training dataset\")\n",
    "\n",
    "                smt = SMOTEENN(sampling_strategy=\"minority\")\n",
    "\n",
    "                input_feature_train_final, target_feature_train_final = smt.fit_resample(\n",
    "                    input_feature_train_arr, target_feature_train_df\n",
    "                )\n",
    "\n",
    "                logging.info(\"Applied SMOTEENN on training dataset\")\n",
    "\n",
    "                logging.info(\"Applying SMOTEENN on testing dataset\")\n",
    "\n",
    "                input_feature_test_final, target_feature_test_final = smt.fit_resample(\n",
    "                    input_feature_test_arr, target_feature_test_df\n",
    "                )\n",
    "\n",
    "                logging.info(\"Applied SMOTEENN on testing dataset\")\n",
    "\n",
    "                logging.info(\"Created train array and test array\")\n",
    "\n",
    "                train_arr = np.c_[\n",
    "                    input_feature_train_final, np.array(target_feature_train_final)\n",
    "                ]\n",
    "\n",
    "                test_arr = np.c_[\n",
    "                    input_feature_test_final, np.array(target_feature_test_final)\n",
    "                ]\n",
    "\n",
    "                save_object(self.data_transformation_config.transformed_object_file_path, preprocessor)\n",
    "                save_numpy_array_data(self.data_transformation_config.transformed_train_file_path, array=train_arr)\n",
    "                save_numpy_array_data(self.data_transformation_config.transformed_test_file_path, array=test_arr)\n",
    "\n",
    "                logging.info(\"Saved the preprocessor object\")\n",
    "\n",
    "                logging.info(\n",
    "                    \"Exited initiate_data_transformation method of Data_Transformation class\"\n",
    "                )\n",
    "\n",
    "                data_transformation_artifact = DataTransformationArtifact(\n",
    "                    transformed_object_file_path=self.data_transformation_config.transformed_object_file_path,\n",
    "                    transformed_train_file_path=self.data_transformation_config.transformed_train_file_path,\n",
    "                    transformed_test_file_path=self.data_transformation_config.transformed_test_file_path\n",
    "                )\n",
    "                return data_transformation_artifact\n",
    "            else:\n",
    "                raise Exception(self.data_validation_artifact.message)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "                   \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
