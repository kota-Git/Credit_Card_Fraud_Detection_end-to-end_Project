{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from from_root import from_root\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_FILE = f\"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\"\n",
    "\n",
    "log_dir = 'logs'\n",
    "\n",
    "logs_path = os.path.join(log_dir, LOG_FILE)\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=logs_path,\n",
    "    format=\"[ %(asctime)s ] %(name)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def error_message_detail(error, error_detail:sys):\n",
    "    _, _, exc_tb = error_detail.exc_info()\n",
    "    file_name = exc_tb.tb_frame.f_code.co_filename\n",
    "    error_message = \"Error occurred python script name [{0}] line number [{1}] error message [{2}]\".format(\n",
    "        file_name, exc_tb.tb_lineno, str(error)\n",
    "    )\n",
    "\n",
    "    return error_message\n",
    "\n",
    "class Credit_card_Exception(Exception):\n",
    "    def __init__(self, error_message, error_detail):\n",
    "        \"\"\"\n",
    "        :param error_message: error message in string format\n",
    "        \"\"\"\n",
    "        super().__init__(error_message)\n",
    "        self.error_message = error_message_detail(\n",
    "            error_message, error_detail=error_detail\n",
    "        )\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.error_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import dill\n",
    "import yaml\n",
    "from pandas import DataFrame\n",
    "\n",
    "from Demo_project.exception import Credit_card_Exception\n",
    "from Demo_project.logger import logging\n",
    "\n",
    "\n",
    "def read_yaml_file(file_path: str) -> dict:\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as yaml_file:\n",
    "            return yaml.safe_load(yaml_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "def write_yaml_file(file_path: str, content: object, replace: bool = False) -> None:\n",
    "    try:\n",
    "        if replace:\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        with open(file_path, \"w\") as file:\n",
    "            yaml.dump(content, file)\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "def load_object(file_path: str) -> object:\n",
    "    logging.info(\"Entered the load_object method of utils\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(file_path, \"rb\") as file_obj:\n",
    "            obj = dill.load(file_obj)\n",
    "\n",
    "        logging.info(\"Exited the load_object method of utils\")\n",
    "\n",
    "        return obj\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "def save_numpy_array_data(file_path: str, array: np.array):\n",
    "    \"\"\"\n",
    "    Save numpy array data to file\n",
    "    file_path: str location of file to save\n",
    "    array: np.array data to save\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(file_path, 'wb') as file_obj:\n",
    "            np.save(file_obj, array)\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "def load_numpy_array_data(file_path: str) -> np.array:\n",
    "    \"\"\"\n",
    "    load numpy array data from file\n",
    "    file_path: str location of file to load\n",
    "    return: np.array data loaded\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file_obj:\n",
    "            return np.load(file_obj)\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "def save_object(file_path: str, obj: object) -> None:\n",
    "    logging.info(\"Entered the save_object method of utils\")\n",
    "\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        with open(file_path, \"wb\") as file_obj:\n",
    "            dill.dump(obj, file_obj)\n",
    "\n",
    "        logging.info(\"Exited the save_object method of utils\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "def drop_columns(df: DataFrame, cols: list)-> DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    drop the columns form a pandas DataFrame\n",
    "    df: pandas DataFrame\n",
    "    cols: list of columns to be dropped\n",
    "    \"\"\"\n",
    "    logging.info(\"Entered drop_columns methon of utils\")\n",
    "\n",
    "    try:\n",
    "        df = df.drop(columns=cols, axis=1)\n",
    "\n",
    "        logging.info(\"Exited the drop_columns method of utils\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise Credit_card_Exception(e, sys) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.constant_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code part  writing in  constant file\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "DATABASE_NAME=\"demo_project_DB\"\n",
    "\n",
    "COLLECTION_NAME=\"fraud_data\"\n",
    "\n",
    "MONGODB_URL_KEY=\"MONGODB_URL\"\n",
    "\n",
    "PIPELINE_NAME:str =\"Demo_project\"  # NOT A SRC its pipeline name\n",
    "\n",
    "ARTIFICAT_DIR:str=\"artificat\"\n",
    "\n",
    "MODEL_FILE_NAME=\"model.pkl\"\n",
    "\n",
    "TARGET_COLUMN=\"default payment next month\"\n",
    "\n",
    "PREPROCESSING_OBJECT_FILE_NAME=\"preprocessing.pkl\"\n",
    "\n",
    "\n",
    "FILE_NAME:str=\"default of credit card data.xls\"\n",
    "\n",
    "TRAIN_FILE_NAME:str=\"train.xls\"\n",
    "\n",
    "TEST_FILE_NAME:str=\"test.xls\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Data Ingestion related constant start with DATA_INGESTION VAR NAME\n",
    "\"\"\"\n",
    "DATA_INGESTION_COLLECTION_NAME: str = \"fraud_data\"\n",
    "DATA_INGESTION_DIR_NAME: str = \"data_ingestion\"\n",
    "DATA_INGESTION_FEATURE_STORE_DIR: str = \"feature_store\"\n",
    "DATA_INGESTION_INGESTED_DIR: str = \"ingested\"\n",
    "DATA_INGESTION_TRAIN_TEST_SPLIT_RATIO: float = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating MongoDBClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in configuration mongo_db_connection.py\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pymongo\n",
    "import certifi\n",
    "from Demo_project.exception import Credit_card_Exception\n",
    "from Demo_project.logger import logging\n",
    "from Demo_project.constants import DATABASE_NAME, MONGODB_URL_KEY\n",
    "\n",
    "ca = certifi.where()\n",
    "\n",
    "class MongoDBClient:\n",
    "    \"\"\"\n",
    "    Class Name :   export_data_into_feature_store\n",
    "    Description :   This method exports the dataframe from mongodb feature store as dataframe \n",
    "    \n",
    "    Output      :   connection to mongodb database\n",
    "    On Failure  :   raises an exception\n",
    "    \"\"\"\n",
    "    client = None\n",
    "\n",
    "    def __init__(self, database_name=DATABASE_NAME) -> None:\n",
    "        try:\n",
    "            if MongoDBClient.client is None:\n",
    "                mongo_db_url = os.getenv(MONGODB_URL_KEY)\n",
    "                if mongo_db_url is None:\n",
    "                    raise Exception(f\"Environment key: {MONGODB_URL_KEY} not set.\")\n",
    "                MongoDBClient.client = pymongo.MongoClient(mongo_db_url,tlsCAFile=ca)\n",
    "            self.client= MongoDBClient.client\n",
    "            self.database = self.client[database_name]\n",
    "            self.database_name = database_name\n",
    "            logging.info(\"Connected to MongoDB database successfull\")\n",
    "        except Exception as e:\n",
    "            raise Credit_card_Exception(e,sys) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: Error occurred python script name [c:\\demo\\demo_project\\Demo_project\\configuration\\mongo_db_connection.py] line number[28] error message [Environment key: MONGODB_URL not set.]\n"
     ]
    }
   ],
   "source": [
    "from Demo_project.configuration.mongo_db_connection import MongoDBClient\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize MongoDB client\n",
    "        mongo_client = MongoDBClient()  # Default uses DATABASE_NAME\n",
    "\n",
    "        # Print connection details for confirmation\n",
    "        print(f\"Connected to MongoDB database: {mongo_client.databse_name}\")\n",
    "        print(f\"Client: {mongo_client.client}\")\n",
    "        \n",
    "        # Accessing a specific collection (example)\n",
    "        collection_name = \"fraud_data\"  # Replace with your collection name\n",
    "        collection = mongo_client.databse[collection_name]\n",
    "        print(f\"Connected to collection: {collection_name}\")\n",
    "\n",
    "        # Fetching and displaying documents from the collection\n",
    "        documents = collection.find()\n",
    "        print(\"Documents in the collection:\")\n",
    "        for doc in documents:\n",
    "            print(doc)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from us_visa.constants import TARGET_COLUMN, SCHEMA_FILE_PATH, CURRENT_YEAR\n",
    "from us_visa.entity.config_entity import DataTransformationConfig\n",
    "from us_visa.entity.artifact_entity import DataTransformationArtifact, DataIngestionArtifact, DataValidationArtifact\n",
    "from us_visa.exception import USvisaException\n",
    "from us_visa.logger import logging\n",
    "from us_visa.utils.main_utils import save_object, save_numpy_array_data, read_yaml_file, drop_columns\n",
    "from us_visa.entity.estimator import TargetValueMapping\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, data_ingestion_artifact: DataIngestionArtifact,\n",
    "                 data_transformation_config: DataTransformationConfig,\n",
    "                 data_validation_artifact: DataValidationArtifact):\n",
    "        \"\"\"\n",
    "        :param data_ingestion_artifact: Output reference of data ingestion artifact stage\n",
    "        :param data_transformation_config: Configuration for data transformation\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_transformation_config = data_transformation_config\n",
    "            self.data_validation_artifact = data_validation_artifact\n",
    "            self._schema_config = read_yaml_file(file_path=SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(file_path) -> pd.DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_to_zero(df, columns):\n",
    "        \"\"\"\n",
    "        Replace invalid values (-2, -1, 0) in specified columns with 0.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for col in columns:\n",
    "                fil = (df[col] == -2) | (df[col] == -1) | (df[col] == 0)\n",
    "                df.loc[fil, col] = 0\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_outliers(df, columns):\n",
    "        \"\"\"\n",
    "        Remove outliers using IQR method for specified columns.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for col in columns:\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_vif(df, numerical_columns):\n",
    "        \"\"\"\n",
    "        Calculate Variance Inflation Factor (VIF) for numerical columns.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            X = df[numerical_columns]\n",
    "            vif_data = pd.DataFrame()\n",
    "            vif_data[\"Feature\"] = X.columns\n",
    "            vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "            return vif_data\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "        \"\"\"\n",
    "        Method to create and return a data transformer object for the data.\n",
    "        \"\"\"\n",
    "        logging.info(\"Entered get_data_transformer_object method of DataTransformation class\")\n",
    "\n",
    "        try:\n",
    "            # Pipeline for numerical columns\n",
    "            numeric_transformer = Pipeline(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),  # Impute missing values for numerical columns\n",
    "                (\"scaler\", StandardScaler())  # Scaling numerical features\n",
    "            ])\n",
    "\n",
    "            # Pipeline for categorical columns\n",
    "            oh_transformer = Pipeline(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Impute missing values for categorical columns\n",
    "                (\"one_hot_encoder\", OneHotEncoder()),  # One-hot encoding for categorical features\n",
    "                (\"scaler\", StandardScaler(with_mean=False))  # Scaling categorical features\n",
    "            ])\n",
    "            \n",
    "            # Ordinal encoding for specified columns\n",
    "            ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "            # Get columns from schema configuration\n",
    "            oh_columns = self._schema_config['oh_columns']\n",
    "            or_columns = self._schema_config['or_columns']\n",
    "            transform_columns = self._schema_config['transform_columns']\n",
    "            num_features = self._schema_config['num_features']\n",
    "\n",
    "            # Power Transformer for feature transformation\n",
    "            transform_pipe = Pipeline(steps=[('transformer', PowerTransformer(method='yeo-johnson'))])\n",
    "\n",
    "            # Combining all transformations using ColumnTransformer\n",
    "            preprocessor = ColumnTransformer([\n",
    "                (\"OneHotEncoder\", oh_transformer, oh_columns),\n",
    "                (\"Ordinal_Encoder\", ordinal_encoder, or_columns),\n",
    "                (\"Transformer\", transform_pipe, transform_columns),\n",
    "                (\"StandardScaler\", numeric_transformer, num_features)\n",
    "            ])\n",
    "\n",
    "            logging.info(\"Created preprocessor object from ColumnTransformer\")\n",
    "            return preprocessor\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    def initiate_data_transformation(self) -> DataTransformationArtifact:\n",
    "        \"\"\"\n",
    "        Method to initiate the data transformation component for the pipeline.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.data_validation_artifact.validation_status:\n",
    "                logging.info(\"Starting data transformation\")\n",
    "\n",
    "                # Read and clean data\n",
    "                train_df = DataTransformation.read_data(file_path=self.data_ingestion_artifact.trained_file_path)\n",
    "                test_df = DataTransformation.read_data(file_path=self.data_ingestion_artifact.test_file_path)\n",
    "\n",
    "                # Replace invalid values in specified columns\n",
    "                target_columns = self._schema_config['replace_invalid_columns']\n",
    "                train_df = self.replace_to_zero(train_df, target_columns)\n",
    "                test_df = self.replace_to_zero(test_df, target_columns)\n",
    "\n",
    "                # Remove duplicates from both train and test data\n",
    "                train_df = train_df.drop_duplicates()\n",
    "                test_df = test_df.drop_duplicates()\n",
    "\n",
    "                # Remove outliers from numerical columns\n",
    "                numerical_columns = self._schema_config['num_features']\n",
    "                train_df = self.remove_outliers(train_df, numerical_columns)\n",
    "                test_df = self.remove_outliers(test_df, numerical_columns)\n",
    "\n",
    "                # VIF Analysis for Multicollinearity Check\n",
    "                vif_data = self.calculate_vif(train_df, numerical_columns)\n",
    "                vif_threshold = 5.0\n",
    "                selected_features = vif_data[vif_data[\"VIF\"] <= vif_threshold][\"Feature\"].tolist()\n",
    "\n",
    "                # Retaining only selected features\n",
    "                train_df = train_df[selected_features + [TARGET_COLUMN]]\n",
    "                test_df = test_df[selected_features + [TARGET_COLUMN]]\n",
    "\n",
    "                # Getting preprocessor object\n",
    "                preprocessor = self.get_data_transformer_object()\n",
    "\n",
    "                # Separating features and target\n",
    "                input_feature_train_df = train_df.drop(columns=[TARGET_COLUMN], axis=1)\n",
    "                target_feature_train_df = train_df[TARGET_COLUMN]\n",
    "\n",
    "                input_feature_test_df = test_df.drop(columns=[TARGET_COLUMN], axis=1)\n",
    "                target_feature_test_df = test_df[TARGET_COLUMN]\n",
    "\n",
    "                # Apply transformations to the train and test data\n",
    "                input_feature_train_arr = preprocessor.fit_transform(input_feature_train_df)\n",
    "                input_feature_test_arr = preprocessor.transform(input_feature_test_df)\n",
    "\n",
    "                # Apply SMOTEENN to handle class imbalance\n",
    "                smt = SMOTEENN(sampling_strategy=\"minority\")\n",
    "                input_feature_train_final, target_feature_train_final = smt.fit_resample(\n",
    "                    input_feature_train_arr, target_feature_train_df\n",
    "                )\n",
    "                input_feature_test_final, target_feature_test_final = smt.fit_resample(\n",
    "                    input_feature_test_arr, target_feature_test_df\n",
    "                )\n",
    "\n",
    "                # Create final train and test arrays\n",
    "                train_arr = np.c_[input_feature_train_final, np.array(target_feature_train_final)]\n",
    "                test_arr = np.c_[input_feature_test_final, np.array(target_feature_test_final)]\n",
    "\n",
    "                # Save preprocessed data and objects\n",
    "                save_object(self.data_transformation_config.transformed_object_file_path, preprocessor)\n",
    "                save_numpy_array_data(self.data_transformation_config.transformed_train_file_path, array=train_arr)\n",
    "                save_numpy_array_data(self.data_transformation_config.transformed_test_file_path, array=test_arr)\n",
    "\n",
    "                # Return the transformation artifact\n",
    "                return DataTransformationArtifact(\n",
    "                    transformed_object_file_path=self.data_transformation_config.transformed_object_file_path,\n",
    "                    transformed_train_file_path=self.data_transformation_config.transformed_train_file_path,\n",
    "                    transformed_test_file_path=self.data_transformation_config.transformed_test_file_path\n",
    "                )\n",
    "            else:\n",
    "                raise Exception(self.data_validation_artifact.message)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataValidation.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "from evidently.model_profile import Profile\n",
    "from evidently.model_profile.sections import DataDriftProfileSection\n",
    "from pandas import DataFrame\n",
    "\n",
    "from us_visa.exception import USvisaException\n",
    "from us_visa.logger import logging\n",
    "from us_visa.utils.main_utils import read_yaml_file, write_yaml_file\n",
    "from us_visa.entity.artifact_entity import DataIngestionArtifact, DataValidationArtifact\n",
    "from us_visa.entity.config_entity import DataValidationConfig\n",
    "from us_visa.constants import SCHEMA_FILE_PATH\n",
    "\n",
    "class DataValidation:\n",
    "    def __init__(self, data_ingestion_artifact: DataIngestionArtifact, data_validation_config: DataValidationConfig):\n",
    "        \"\"\"\n",
    "        :param data_ingestion_artifact: Output reference of data ingestion artifact stage\n",
    "        :param data_validation_config: configuration for data validation\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_validation_config = data_validation_config\n",
    "            self._schema_config = read_yaml_file(file_path=SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    def validate_number_of_columns(self, dataframe: DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Method Name :   validate_number_of_columns\n",
    "        Description :   This method validates the number of columns\n",
    "        \n",
    "        Output      :   Returns bool value based on validation results\n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        try:\n",
    "            status = len(dataframe.columns) == len(self._schema_config[\"columns\"])\n",
    "            logging.info(f\"Is required column present: [{status}]\")\n",
    "            return status\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    def is_column_exist(self, df: DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Method Name :   is_column_exist\n",
    "        Description :   This method validates the existence of a numerical and categorical columns\n",
    "        \n",
    "        Output      :   Returns bool value based on validation results\n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dataframe_columns = df.columns\n",
    "            missing_numerical_columns = []\n",
    "            missing_categorical_columns = []\n",
    "            for column in self._schema_config[\"numerical_columns\"]:\n",
    "                if column not in dataframe_columns:\n",
    "                    missing_numerical_columns.append(column)\n",
    "\n",
    "            if len(missing_numerical_columns) > 0:\n",
    "                logging.info(f\"Missing numerical column: {missing_numerical_columns}\")\n",
    "\n",
    "            for column in self._schema_config[\"categorical_columns\"]:\n",
    "                if column not in dataframe_columns:\n",
    "                    missing_categorical_columns.append(column)\n",
    "\n",
    "            if len(missing_categorical_columns) > 0:\n",
    "                logging.info(f\"Missing categorical column: {missing_categorical_columns}\")\n",
    "\n",
    "            return False if len(missing_categorical_columns) > 0 or len(missing_numerical_columns) > 0 else True\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys) from e\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(file_path) -> DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys)\n",
    "\n",
    "    def detect_dataset_drift(self, reference_df: DataFrame, current_df: DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Method Name :   detect_dataset_drift\n",
    "        Description :   This method validates if drift is detected\n",
    "        \n",
    "        Output      :   Returns bool value based on validation results\n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_drift_profile = Profile(sections=[DataDriftProfileSection()])\n",
    "            data_drift_profile.calculate(reference_df, current_df)\n",
    "\n",
    "            report = data_drift_profile.json()\n",
    "            json_report = json.loads(report)\n",
    "\n",
    "            write_yaml_file(file_path=self.data_validation_config.drift_report_file_path, content=json_report)\n",
    "\n",
    "            n_features = json_report[\"data_drift\"][\"data\"][\"metrics\"][\"n_features\"]\n",
    "            n_drifted_features = json_report[\"data_drift\"][\"data\"][\"metrics\"][\"n_drifted_features\"]\n",
    "\n",
    "            logging.info(f\"{n_drifted_features}/{n_features} drift detected.\")\n",
    "            drift_status = json_report[\"data_drift\"][\"data\"][\"metrics\"][\"dataset_drift\"]\n",
    "            return drift_status\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys) from e\n",
    "\n",
    "    def initiate_data_validation(self) -> DataValidationArtifact:\n",
    "        \"\"\"\n",
    "        Method Name :   initiate_data_validation\n",
    "        Description :   This method initiates the data validation component for the pipeline\n",
    "        \n",
    "        Output      :   Returns bool value based on validation results\n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        try:\n",
    "            validation_error_msg = \"\"\n",
    "            logging.info(\"Starting data validation\")\n",
    "            train_df, test_df = (DataValidation.read_data(file_path=self.data_ingestion_artifact.trained_file_path),\n",
    "                                 DataValidation.read_data(file_path=self.data_ingestion_artifact.test_file_path))\n",
    "\n",
    "            # Validate the number of columns\n",
    "            status = self.validate_number_of_columns(dataframe=train_df)\n",
    "            logging.info(f\"All required columns present in training dataframe: {status}\")\n",
    "            if not status:\n",
    "                validation_error_msg += f\"Columns are missing in training dataframe.\"\n",
    "            status = self.validate_number_of_columns(dataframe=test_df)\n",
    "            logging.info(f\"All required columns present in testing dataframe: {status}\")\n",
    "            if not status:\n",
    "                validation_error_msg += f\"Columns are missing in test dataframe.\"\n",
    "\n",
    "            # Validate if all necessary columns exist\n",
    "            status = self.is_column_exist(df=train_df)\n",
    "            if not status:\n",
    "                validation_error_msg += f\"Columns are missing in training dataframe.\"\n",
    "            status = self.is_column_exist(df=test_df)\n",
    "            if not status:\n",
    "                validation_error_msg += f\"columns are missing in test dataframe.\"\n",
    "\n",
    "            # Check for duplicates\n",
    "            if train_df.duplicated().any():\n",
    "                logging.info(\"Duplicate rows found in training data\")\n",
    "                validation_error_msg += f\"Duplicate rows found in training data.\"\n",
    "            if test_df.duplicated().any():\n",
    "                logging.info(\"Duplicate rows found in test data\")\n",
    "                validation_error_msg += f\"Duplicate rows found in test data.\"\n",
    "\n",
    "            # Check for missing values\n",
    "            if train_df.isnull().sum().any():\n",
    "                logging.info(f\"Missing values in training data: {train_df.isnull().sum()}\")\n",
    "                validation_error_msg += f\"Missing values found in training data.\"\n",
    "            if test_df.isnull().sum().any():\n",
    "                logging.info(f\"Missing values in test data: {test_df.isnull().sum()}\")\n",
    "                validation_error_msg += f\"Missing values found in test data.\"\n",
    "\n",
    "            # Check data types\n",
    "            for column in self._schema_config[\"columns\"]:\n",
    "                if column in train_df.columns and train_df[column].dtype != self._schema_config[\"columns\"][column]:\n",
    "                    logging.info(f\"Data type mismatch for column {column} in training data\")\n",
    "                    validation_error_msg += f\"Data type mismatch for column {column} in training data.\"\n",
    "                if column in test_df.columns and test_df[column].dtype != self._schema_config[\"columns\"][column]:\n",
    "                    logging.info(f\"Data type mismatch for column {column} in test data\")\n",
    "                    validation_error_msg += f\"Data type mismatch for column {column} in test data.\"\n",
    "\n",
    "            # Rename columns according to schema (if needed)\n",
    "            column_rename_mapping = self._schema_config.get('column_rename', {})\n",
    "            if column_rename_mapping:\n",
    "                train_df.rename(columns=column_rename_mapping, inplace=True)\n",
    "                test_df.rename(columns=column_rename_mapping, inplace=True)\n",
    "                logging.info(f\"Renamed columns in train and test data: {column_rename_mapping}\")\n",
    "\n",
    "            validation_status = len(validation_error_msg) == 0\n",
    "\n",
    "            # If validation passes, check for drift\n",
    "            if validation_status:\n",
    "                drift_status = self.detect_dataset_drift(train_df, test_df)\n",
    "                if drift_status:\n",
    "                    logging.info(f\"Drift detected.\")\n",
    "                    validation_error_msg = \"Drift detected\"\n",
    "                else:\n",
    "                    validation_error_msg = \"Drift not detected\"\n",
    "            else:\n",
    "                logging.info(f\"Validation_error: {validation_error_msg}\")\n",
    "\n",
    "            # Create the DataValidationArtifact\n",
    "            data_validation_artifact = DataValidationArtifact(\n",
    "                validation_status=validation_status,\n",
    "                message=validation_error_msg,\n",
    "                drift_report_file_path=self.data_validation_config.drift_report_file_path\n",
    "            )\n",
    "\n",
    "            logging.info(f\"Data validation artifact: {data_validation_artifact}\")\n",
    "            return data_validation_artifact\n",
    "        except Exception as e:\n",
    "            raise USvisaException(e, sys) from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
